# -*- coding: utf-8 -*-
"""13-1. HSanaei_ML_HW_Chap13_Analyzing_Movie_Review_RNNs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KX5QutusujqlJFnZFtf2tguJtvGUNEn4
"""

'''
Python Machine Learning
Teacher: Dr Rahmani
Student: Hossein SANAEI ~حسين سنايي

Aras International Campus of University of Tehran
Spring 1401 (2022)
GitHub: https://github.com/HSanaei/DeepLearing.git

Chapter 13:  Analyzing movie review sentiment with RNNs

'''
# first importing all necessary modules from tensorflow
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras import layers, models, losses, optimizers
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Loading Keras built-in IMDb dataset
vocab_size = 5000
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)

# Take a look at the training and testing data we just loaded
print('Number of training samples:', len(y_train))
print('Number of positive samples', sum(y_train))
print('Number of test samples:', len(y_test))

# Print a training sample, as follows:
print(X_train[0])

#We use the word dictionary to map the integer back to the word it represents:
word_index = imdb.get_word_index()
index_word = {index: word for word, index in word_index.items()}

#Take the first review as an example:
print([index_word.get(i, ' ') for i in X_train[0]])

''' Next, we analyze the length of each sample (the number of words in each
review, for example). We do so because all the input sequences to an RNN
model must be the same length:
'''

review_lengths = [len(x) for x in X_train]
#Plot the distribution of these document lengths, as follows:
import matplotlib.pyplot as plt

plt.hist(review_lengths, bins=10)
plt.show()

# We will see that the majority of the reviews are around 200 words long.

''' Next, we set 200 as the universal sequence length by padding shorter reviews with
zeros and truncating longer reviews. We use the pad_sequences function
from Keras to accomplish this:
'''

maxlen = 200
X_train = pad_sequences(X_train, maxlen=maxlen)
X_test = pad_sequences(X_test, maxlen=maxlen)

# Let's look at the shape of the input sequences after this:
print('X_train shape after padding:', X_train.shape)
print('X_test shape after padding:', X_test.shape)

# Building a simple LSTM network
# First, we fix the random seed and initiate a Keras Sequential model:
tf.random.set_seed(42)
model = models.Sequential()

''' Since our input sequences are word indices that are equivalent to onehot
encoded vectors, we need to embed them in dense vectors using the
Embedding layer from Keras:
'''

embedding_size = 32
model.add(layers.Embedding(vocab_size, embedding_size))

''' Here, we embed the input sequences that are made of up vocab_size=5000
unique word tokens into dense vectors of size 32.
'''
# Now here comes the recurrent layer, the LSTM layer specifically:
model.add(layers.LSTM(50))

''' After that, we add the output layer, along with a sigmoid activation function,
since we are working on a binary classification problem:
'''
model.add(layers.Dense(1, activation='sigmoid'))
# Display the model summary to double-check the layers:
print(model.summary())

''' Next, we compile the model with the Adam optimizer and use binary crossentropy
as the optimization target:
'''
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Finally, we train the model with batches of size 64 for three epochs:
batch_size = 64
n_epoch = 3
model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epoch, validation_data=(X_test, y_test))

# Using the trained model, we evaluate the classification accuracy on the testing set:
acc = model.evaluate(X_test, y_test, verbose = 0)[1]
print('Test accuracy:', acc)

# Stacking multiple LSTM layers

''' Let's see whether we can beat the previous accuracy by following these steps to
build a multi-layer RNN model:

First: Initiate a new model and add an embedding layer, two LSTM layers, and an
output layer:
'''

model = models.Sequential()
model.add(layers.Embedding(vocab_size, embedding_size))
model.add(layers.LSTM(50, return_sequences=True, dropout=0.2))
model.add(layers.LSTM(50, dropout=0.2))
model.add(layers.Dense(1, activation='sigmoid'))

''' Here, the first LSTM layer comes with return_sequences=True as we need to
feed its entire output sequence to the second LSTM layer. We also add 20%
dropout to both LSTM layers to reduce overfitting since we will have more
parameters to train:
'''
print(model.summary())

# Similarly, we compile the model with the Adam optimizer at a 0.003 learning rate:
optimizer = optimizers.Adam(lr=0.003)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Then, we train the stacked model for 7 epochs:
n_epoch = 7
model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epoch, validation_data=(X_test, y_test))

# Finally, we verify the test accuracy:
acc = model.evaluate(X_test, y_test, verbose=0)[1]
print('Test accuracy with stacked LSTM:', acc)