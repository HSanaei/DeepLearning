# -*- coding: utf-8 -*-
"""14-2. HosseinSanaei-DeepLearning-HW-Ch14-Solving-FrozenLake-With-VIA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DxWk02tHY-nm6T_V_yzXX_EPCK9yV8Oy
"""

'''
Python Machine Learning
Teacher: Dr Rahmani
Student: Hossein SANAEI ~Õ”Ì‰ ”‰«Ì?

Aras International Campus of University of Tehran
Spring 1401 (2022)
GitHub: https://github.com/HSanaei/DeepLearing.git

Chapter 14-2:  Solving FrozenLake with value iteration algorithm

'''

# We import the Gym library and create a FrozenLake instance:

import gym
import torch

env = gym.make("FrozenLake-v0")
gamma = 0.99
threshold = 0.0001

# We develop the value iteration algorithm, which computes the optimal values:

def value_iteration(env, gamma, threshold):
    """
    Solve a given environment with value iteration algorithm
    @param env: OpenAI Gym environment
    @param gamma: discount factor
    @param threshold: the evaluation will stop once values
    for all states are less than the threshold
    @return: values of the optimal policy for the given environment
    """
    n_state = env.observation_space.n
    n_action = env.action_space.n
    V = torch.zeros(n_state)
    while True:
          V_temp = torch.empty(n_state)
          for state in range(n_state):
              v_actions = torch.zeros(n_action)
              for action in range(n_action):
                  for trans_prob, new_state, reward, _ in \
                                  env.env.P[state][action]:
                      v_actions[action] += trans_prob * (reward + gamma * V[new_state])
              V_temp[state] = torch.max(v_actions)
          max_delta = torch.max(torch.abs(V - V_temp))
          V = V_temp.clone()
          if max_delta <= threshold:
             break
    return V

''' We apply the algorithm to solve the FrozenLake environment along with the
specified parameters:
'''
V_optimal = value_iteration(env, gamma, threshold)
# Take a look at the resulting optimal values:
print('Optimal values:\n', V_optimal)

'''
Since we have the optimal values, we can extract the optimal policy from the
values. We develop the following function to do this:
'''
def extract_optimal_policy(env, V_optimal, gamma):
    '''
    Obtain the optimal policy based on the optimal values
    @param env: OpenAI Gym environment
    @param V_optimal: optimal values
    @param gamma: discount factor
    @return: optimal policy
    '''
    n_state = env.observation_space.n
    n_action = env.action_space.n
    optimal_policy = torch.zeros(n_state)
    for state in range(n_state):
        v_actions = torch.zeros(n_action)
        for action in range(n_action):
            for trans_prob, new_state, reward, _ in \
                            env.env.P[state][action]:
                v_actions[action] += trans_prob * (
                       reward + gamma *
                               V_optimal[new_state])
        optimal_policy[state] = torch.argmax(v_actions)
    return optimal_policy

# Then we obtain the optimal policy based on the optimal values:
optimal_policy = extract_optimal_policy(env, V_optimal, gamma)
# Take a look at the resulting optimal policy:
print('Optimal policy:\n', optimal_policy)

'''
If you doubt that it is the optimal policy, you can run 1,000 episodes with the
policy and gauge how good it is by checking the average reward, as follows:
'''
def run_episode(env, policy):
    state = env.reset()
    total_reward = 0
    is_done = False
    while not is_done:
        action = policy[state].item()
        state, reward, is_done, info = env.step(action)
        total_reward += reward
        if is_done:
            break
    return total_reward

n_episode = 1000
total_rewards = []
for episode in range(n_episode):
    total_reward = run_episode(env, optimal_policy)
    total_rewards.append(total_reward)

'''
Here, we reuse the run_episode function we defined in the previous section.
Then we print out the average reward:
'''
print('Average total reward under the optimal policy:', 
      sum(total_rewards) / n_episode)