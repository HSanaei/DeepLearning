# -*- coding: utf-8 -*-
"""14-5. HosseinSanaei-DeepLearning-HW-Ch14-Performing-on_Policy-MonteCarlo-Control.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yLsHAE_XxJKntSip7phfnjzKN8iSbZBw
"""

'''
Python Machine Learning
Teacher: Dr Rahmani
Student: Hossein SANAEI ~Ø­Ø³ÙŠÙ† Ø³Ù†Ø§ÙŠÙŠ

Aras International Campus of University of Tehran
Spring 1401 (2022)
GitHub: https://github.com/HSanaei/DeepLearing.git

Chapter 14-5:  Performing on-policy Monte Carlo control
'''
import torch
import gym

env = gym.make('Blackjack-v0')

'''1. We start with developing a function that executes an episode by taking the
best actions under the given Q-values:
'''
def run_episode(env, Q, n_action):
    '''
    Run a episode given Q-values
    @param env: OpenAI Gym environment
    @param Q: Q-values
    @param n_action: action space
    @return: resulting states, actions and rewards for the
    entire episode
    '''
    state = env.reset()
    rewards = []
    actions = []
    states = []
    action = torch.randint(0, n_action, [1]).item()
    while True:
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
        if is_done:
           break
        action = torch.argmax(Q[state]).item()
    return states, actions, rewards
'''
This serves as the improvement phase. Specifically, it does the following tasks:
â€¢ Initializing an episode
â€¢ Taking a random action as an exploring start
â€¢ After the first action, taking actions based on the given Q-value table,
  that is ğ‘ğ‘ = ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘¥ğ‘¥ğ‘ğ‘ğ‘„ğ‘„(ğ‘ ğ‘ , ğ‘ğ‘)
â€¢ Storing the states, actions, and rewards for all steps in the episode,
  which will be used for evaluation
'''

# 2. Next, we develop the on-policy MC control algorithm:
def mc_control_on_policy(env, gamma, n_episode):
    '''
    @Obtain the optimal policy with on-policy MC control method
    @param env: OpenAI Gym environment
    @param gamma: discount factor
    @param n_episode: number of episodes
    @return: the optimal Q-function, and the optimal policy
    '''
    from collections import defaultdict
    G_sum = defaultdict(float)
    N = defaultdict(int)
    Q = defaultdict(lambda: torch.empty(env.action_space.n))
    for episode in range(n_episode):
        states_t, actions_t, rewards_t = run_episode(env, Q, env.action_space.n)
        return_t = 0
        G = {}
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1],
                                                               rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            G[(state_t, action_t)] = return_t
        for state_action, return_t in G.items():
            state, action = state_action
            if state[0] <= 21:
               G_sum[state_action] += return_t
               N[state_action] += 1
               Q[state][action] = G_sum[state_action] / N[state_action]
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy
'''
This function does the following tasks:
â€¢ Randomly initializing the Q-values
â€¢ Running n_episode episodes
â€¢ For each episode, performing policy improvement and obtaining the
training data; performing first-visit policy evaluation on the resulting
states, actions, and rewards, and updating the Q-values
â€¢ In the end, finalizing the optimal Q-values and the optimal policy
'''

# 3. Now that the MC control function is ready, we compute the optimal policy:
gamma = 1
n_episode = 500000

optimal_Q, optimal_policy = mc_control_on_policy(env, gamma, n_episode)

# Take a look at the optimal policy:
print(optimal_policy)

'''
You may wonder if this optimal policy is really optimal and better than the previous
simple policy (hold at 18 points). Let's simulate 100,000 Blackjack episodes under the
optimal policy and the simple policy respectively:
'''
# 1. We start with the function that simulates an episode under the simple policy:
def simulate_hold_episode(env, hold_score):
    state = env.reset()
    while True:
        action = 1 if state[0] < hold_score else 0
        state, reward, is_done, _ = env.step(action)
        if is_done:
           return reward

# 2. Next, we work on the simulation function under the optimal policy:
def simulate_episode(env, policy):
    state = env.reset()
    while True:
        action = policy[state]
        state, reward, is_done, _ = env.step(action)
        if is_done:
           return reward

# We then run 100,000 episodes for both policies and keep track of their winning times:
n_episode = 100000
hold_score = 18
n_win_opt = 0
n_win_hold = 0
for _ in range(n_episode):
    reward = simulate_episode(env, optimal_policy)
    if reward == 1:
       n_win_opt += 1
    reward = simulate_hold_episode(env, hold_score)
    if reward == 1:
       n_win_hold += 1

# We print out the results as follows:
print(f'Winning probability:\nUnder the simple policy: {n_win_hold/n_episode}\nUnder the optimal policy: {n_win_opt/n_episode}')