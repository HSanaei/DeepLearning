# -*- coding: utf-8 -*-
"""14-4. HosseinSanaei-DeepLearning-HW-Ch14-Simulating-the-BlackJack-Environment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17R9nXIOn_Il4n7HgLO3TThailTShy_-N
"""

'''
Python Machine Learning
Teacher: Dr Rahmani
Student: Hossein SANAEI ~حسين سنايي

Aras International Campus of University of Tehran
Spring 1401 (2022)
GitHub: https://github.com/HSanaei/DeepLearing.git

Chapter 14-4:  Performing Monte Carlo Learning - Simulating the Blackjack environment
'''
import torch
import gym

# 1. First create a Blackjack instance:
env = gym.make('Blackjack-v0')

# 2. Reset the environment:
env.reset()

'''
3. Let's now take some actions to see how the environment works. First, we take
a hit action since we only have 7 points:
'''
env.step(1)

# 4. Let's take another hit since we only have 13 points:
env.step(1)

# 5. We have 19 points and think it is good enough. Then we stop drawing cards
# by taking action stick (0):
env.step(0)

# Performing Monte Carlo policy evaluation
# 1. We first need to define a function that simulates a Blackjack episode under
# the simple policy:
def run_episode(env, hold_score):
    state = env.reset()
    rewards = []
    states = [state]
    while True:
        action = 1 if state[0] < hold_score else 0
        state, reward, is_done, info = env.step(action)
        states.append(state)
        rewards.append(reward)
        if is_done:
            break
    return states, rewards

# In each round of an episode, the agent takes a hit if the current score is less
# than hold_score or a stick otherwise.

'''
2. In the MC settings, we need to keep track of states and rewards over all
steps. And in first-visit value evaluation, we average returns only for the first
occurrence of a state among all episodes. We define a function that evaluates
the simple Blackjack policy with first-visit MC:
'''
from collections import defaultdict

def mc_prediction_first_visit(env, hold_score, gamma, n_episode):
    V = defaultdict(float)
    N = defaultdict(int)
    for episode in range(n_episode):
        states_t, rewards_t = run_episode(env, hold_score)
        return_t = 0
        G = {}
        for state_t, reward_t in zip(
                     states_t[1::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            G[state_t] = return_t
        for state, return_t in G.items():
            if state[0] <= 21:
                V[state] += return_t
                N[state] += 1
    for state in V:
        V[state] = V[state] / N[state]
    return V

'''
The function performs the following tasks:
• Running n_episode episodes under the simple Blackjack policy with
function run_episode
• For each episode, computing the returns G for the first visit of each
state
• For each state, obtaining the value by averaging its first returns from
all episodes
• Returning the resulting values
Note that here we ignore states where the player busts, since we know their
values are -1.
'''

'''
3. We specify the hold_score as 18, the discount rate as 1 as a Blackjack episode
is short enough, and will simulate 500,000 episodes:
'''
hold_score = 18
gamma = 1
n_episode = 500000

#4. Now we plug in all variables to perform MC first-visit evaluation:
value = mc_prediction_first_visit(env, hold_score, gamma, n_episode)
# We then print the resulting values:
print(value)

# We have just computed the values for all possible 280 states:
print('Number of stat-s:', len(value))