# -*- coding: utf-8 -*-
"""14-7. HosseinSanaei-DeepLearning-HW-Ch14-Developping-QLearning-Algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a1mimbstI1awNuqYCfbdcF97JeJr2mf8
"""

'''
Python Machine Learning
Teacher: Dr Rahmani
Student: Hossein SANAEI ~حسين سنايي

Aras International Campus of University of Tehran
Spring 1401 (2022)
GitHub: https://github.com/HSanaei/DeepLearing.git

Chapter 14-7:  Developping the Q-Learning Algorithm
'''
import torch
import gym
env = gym.make('Taxi-v3')

# 1. We start with defining the epsilon-greedy policy:
def gen_epsilon_greedy_policy(n_action, epsilon):
    def policy_function(state, Q):
        probs = torch.ones(n_action) * epsilon / n_action
        best_action = torch.argmax(Q[state]).item()
        probs[best_action] += 1.0 - epsilon
        action = torch.multinomial(probs, 1).item()
        return action
    return policy_function
'''
Given |A| possible actions, each action is taken with a probability 𝜀𝜀⁄𝐴𝐴 ∨ , and
the action with the highest state-action value is chosen with an additional
probability 1 − 𝜀𝜀 .
'''

# Now we create an instance of the epsilon-greedy-policy:
epsilon = 0.1
epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space.n, epsilon)
# Here, 𝜀𝜀 = 0.1 , which is the exploration ratio.

# 3. Next, we develop the Q-learning algorithm:
from collections import defaultdict

def q_learning(env, gamma, n_episode, alpha):
    '''
    Obtain the optimal policy with off-policy Q-learning method
    @param env: OpenAI Gym environment
    @param gamma: discount factor
    @param n_episode: number of episodes
    @return: the optimal Q-function, and the optimal policy
    '''
    n_action = env.action_space.n
    Q = defaultdict(lambda: torch.zeros(n_action))
    for episode in range(n_episode):
        state = env.reset()
        is_done = False
        while not is_done:
            action = epsilon_greedy_policy(state, Q)
            next_state, reward, is_done, info = env.step(action)
            delta = reward + gamma * torch.max(Q[next_state]) - Q[state][action]
            Q[state][action] += alpha * delta
            length_episode[episode] += 1
            total_reward_episode[episode] += reward
            if is_done:
               break
            state = next_state
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy
'''
We first initialize the Q-table. Then in each episode, we let the agent take
actions following the epsilon-greedy policy, and update the Q function
for each step based on the off-policy learning equation. We run n_episode
episodes and finally obtain the optimal policy and Q-values.
'''

'''
4. We then initiate two variables to store the performance of each of 1,000
episodes, the episode length (number of steps in an episode), and total
reward:
'''
n_episode = 1000
length_episode = [0] * n_episode
total_reward_episode = [0] * n_episode

# 5. Finally, we perform Q-learning to obtain the optimal policy for the Taxi problem:
gamma = 1
alpha = 0.4
optimal_Q, optimal_policy = q_learning(env, gamma, n_episode, alpha)

# Here, discount rate 𝛾𝛾 = 1 , and learning rate 𝛼𝛼 = 0.4 .

# After 1,000 episodes of learning, we plot the total rewards over episodes as follows:

import matplotlib.pyplot as plt

plt.plot(total_reward_episode)
plt.title('Episode reward over time')
plt.xlabel('Episode')
plt.ylabel('Total reward')
plt.ylim([-200, 20])
plt.show()

"""The total rewards keep improving during learning. And they stay around +5
after 600 episodes.
"""

# We also plot the lengths over episodes as follows:
plt.plot(length_episode)
plt.title('Episode length over time')
plt.xlabel('Episode')
plt.ylabel('Length')
plt.show()

"""As you can see, the episode lengths decrease from the maximum 200 to
around 10, and the model converges around 600 episodes. It means after
training, the model is able to solve the problem in around 10 steps.
In this section, we solved the Taxi problem with off-policy Q-learning. The algorithm
optimizes the Q-values in every single step by learning from the experience
generated by a greedy policy.
"""