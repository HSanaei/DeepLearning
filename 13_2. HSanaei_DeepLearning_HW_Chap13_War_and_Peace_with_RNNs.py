# -*- coding: utf-8 -*-
"""13-2. HSanaei_ML_HW_Chap13_War_and_Peace_with_RNNs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18H0DRoEnlJVAIUKJEyS71_yrVm7Wvlrp
"""

'''
Python Machine Learning
Teacher: Dr Rahmani
Student: Hossein SANAEI ~حسين سنايي

Aras International Campus of University of Tehran
Spring 1401 (2022)
GitHub: https://github.com/HSanaei/DeepLearing.git

Chapter 13:  Writing your own War and Peace with RNNs

'''

'''The original work, War and Peace, can be downloaded from http://www.gutenberg.
org/ebooks/2600, but note that there will be some cleanup, such as removing the
extra beginning section "The Project Gutenberg EBook," the table of contents, and the
extra appendix "End of the Project Gutenberg EBook of War and Peace" of the plain
text UTF-8 file (http://www.gutenberg.org/files/2600/2600-0.txt) required. So,
instead of doing this, we will download the cleaned text file directly from https://
cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt. Let's get started:
'''

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras import layers, models, losses, optimizers
from tensorflow.keras.preprocessing.sequence import pad_sequences

# First, we read the file and convert the text into lowercase:
training_file = '/content/drive/MyDrive/Colab Notebooks/warpeace_input.txt'
raw_text = open(training_file, 'r').read()
raw_text = raw_text.lower()

#Then, we take a quick look at the training text data by printing out the first 200 characters:
print(raw_text[:200])

# Next, we count the number of unique words:
all_words = raw_text.split()
unique_words = list(set(all_words))
print(f'Number of unique words: {len(unique_words)}')

#And then, we count the total number of characters:
n_chars = len(raw_text)
print(f'Total characters: {n_chars}')

# From these 3 million characters, we obtain the unique characters, as follows:
chars = sorted(list(set(raw_text)))
n_vocab = len(chars)
print(f'Total vocabulary (unique characters): {n_vocab}')
print(chars)

''' We also need to one-hot encode the input and output characters since neural
network models only take in numerical data. We simply map the 57 unique
characters to indices from 0 to 56, as follows:
'''
index_to_char = dict((i, c) for i, c in enumerate(chars))
char_to_index = dict((c, i) for i, c in enumerate(chars))
print(char_to_index)

''' Now that the character lookup dictionary is ready, we can construct the entire
training set, as follows:
'''
import numpy as np

seq_length = 100
n_seq = int(n_chars / seq_length)


'''Here, we set the sequence length to 160 and obtain n_seq training samples. Next, we
initialize the training inputs and outputs, which are both of the shape (number of
samples, sequence length, feature dimension):
'''
X = np.zeros((n_seq, seq_length, n_vocab))
Y = np.zeros((n_seq, seq_length, n_vocab))

''' Now, for each of the n_seq samples, we assign "1" to the indices of the input and
output vectors where the corresponding characters exist:
'''
for i in range(n_seq):
    x_sequence = raw_text[i * seq_length :(i + 1) * seq_length]
    x_sequence_ohe = np.zeros((seq_length, n_vocab))
    for j in range(seq_length):
            char = x_sequence[j]
            index = char_to_index[char]
            x_sequence_ohe[j][index] = 1.
    X[i] = x_sequence_ohe

    y_sequence = raw_text[i * seq_length + 1 : (i + 1) * seq_length + 1]
    y_sequence_ohe = np.zeros((seq_length, n_vocab))
    for j in range(seq_length):
            char =y_sequence[j]
            index = char_to_index[char]                       
            y_sequence_ohe[j][index] = 1.
    Y[i] = y_sequence_ohe

# Next, take a look at the shapes of the constructed input and output samples:
X.shape
Y.shape

# Building an RNN text generator
# First, we import all the necessary modules and fix a random seed:

import tensorflow as tf
from tensorflow.keras import layers, models, losses, optimizers

tf.random.set_seed(42)

#Each recurrent layer contains 500 units, with a 0.4 dropout ratio and a tanh activation function:
hidden_units = 500
dropout = 0.4

# We specify other hyperparameters, including the batch size, 100, and the number of epochs, 50:
batch_size = 100
n_epoch= 50

# Now, we create the RNN model, as follows:
model = models.Sequential()
model.add(layers.LSTM(hidden_units, input_shape=(None, n_vocab), return_sequences=True, dropout=dropout))
model.add(layers.LSTM(hidden_units, return_sequences=True, dropout=dropout))
model.add(layers.TimeDistributed(layers.Dense(n_vocab, activation='softmax')))

# Next, we compile the network. As for the optimizer, we choose RMSprop with a learning rate of 0.001:
optimizer = optimizers.RMSprop(lr=0.001)
model.compile(loss="categorical_crossentropy", optimizer=optimizer)

# Let's summarize the model we just built:
print(model.summary())

# Training the RNN text generator

# First, we import the necessary modules:
from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping

# Then, we define the model checkpoint callback:
file_path = "weights/weights_epoch_{epoch:03d}_loss_{loss:.4f}.hdf5"
checkpoint = ModelCheckpoint(file_path, monitor='loss', verbose=1, save_best_only=True, mode='min')

''' After that, we create an early stopping callback to halt the training if the 
validation loss doesn't decrease for 50 successive epochs:
'''
early_stop = EarlyStopping(monitor='loss', min_delta=0, patience=50, verbose=1, mode='min')

# Next, we develop a helper function that generates text of any length, given a model:
def generate_text(model, gen_length, n_vocab, index_to_char):
   """
   Generating text using the RNN model
   @param model: current RNN model
   @param gen_length: number of characters we want to generate
   @param n_vocab: number of unique characters
   @param index_to_char: index to character mapping
   @return: string of text generated
   """
# Start with a randomly picked character
   index = np.random.randint(n_vocab)
   y_char = [index_to_char[index]]
   X = np.zeros((1, gen_length, n_vocab))
   for i in range(gen_length):
        X[0, i, index] = 1.
        indices = np.argmax(model.predict(X[:, max(0, i - seq_length -1):i + 1, :])[0], 1)
        index = indices[-1]
        y_char.append(index_to_char[index])
   return ''.join(y_char)

''' Now, we define the callback class that generates text with the generate_text
util function for every N epochs:
'''
class ResultChecker(Callback):
    def __init__(self, model, N, gen_length):
        self.model = model
        self.N = N
        self.gen_length = gen_length
        
    def on_epoch_end(self, epoch, logs={}):
        if epoch % self.N == 0:
            result = generate_text(self.model,
                     self.gen_length, n_vocab, index_to_char)
            print('\nMy War and Peace:\n' + result)

# Next, we initiate a text generation checker callback:
result_checker = ResultChecker(model, 10, 500)

# Now that all the callback components are ready, we can start training the model:
model.fit(X, Y, batch_size=batch_size,
verbose=1, epochs=n_epoch,callbacks=[result_checker, checkpoint, early_stop])