# -*- coding: utf-8 -*-
"""14-1. HosseinSanaei-DeepLearning-HW-Ch14-Simulating-FrozenLake-Environment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CX6l6i55_Bi2ZuOHCX33BMv5hx-83cn9
"""

'''
Python Machine Learning
Teacher: Dr Rahmani
Student: Hossein SANAEI ~ÍÓíä ÓäÇí?

Aras International Campus of University of Tehran
Spring 1401 (2022)
GitHub: https://github.com/HSanaei/DeepLearing.git

Chapter 14-1:  FrozenLake environment with dynamic programming
               Simulating the Frozen Lake environment

'''

# We import the Gym library and create a FrozenLake instance:

import gym
import torch

env = gym.make("FrozenLake-v0")
n_state = env.observation_space.n
print(n_state)
print("\n")

n_action = env.action_space.n
print(n_action)
print("\n")

# Every time we run a new episode, we need to reset the environment:
env.reset()

''' It means that the agent starts with state 0. Again, there are 16 possible states,
0, 1, â€¦, 15.
We render the environment to display it:
'''
env.render()

# Let's take a right action since it is walkable:
new_state, reward, is_done, info = env.step(2)
print(new_state)
print(reward)
print(is_done)
print(info)

'''The agent moves right to state 1, at a probability of 33.33%, and gets 0 reward
since the episode is not done yet. Also see the render result:
'''
env.render()

"""You may get a completely different result as the agent can move down to
state 4 at a probability of 33.33%, or stay at state 0 at a probability of 33.33%.
"""

'''
6. Next, we define a function that simulates a FrozenLake episode under a
given policy and returns the total reward (as an easy start, let's just assume
discount factor ğ›¾ = 1 ):
'''

def run_episode(env, policy):
    state = env.reset()
    total_reward = 0
    is_done = False
    while not is_done:
        action = policy[state].item()
        state, reward, is_done, info = env.step(action)
        total_reward += reward
        if is_done:
           break
    return total_reward

'''
7. Now let's play around with the environment using a random policy. We will
implement a random policy (where random actions are taken) and calculate
the average total reward over 1,000 episodes:
'''
# import torch

n_episode = 1000
total_rewards = []
for episode in range(n_episode):
    random_policy = torch.randint(high=n_action, size=(n_state,))
    total_reward = run_episode(env, random_policy)
    total_rewards.append(total_reward)
print(f'Average total reward under random policy: {sum(total_rewards)/n_episode}')

'''
8. As a bonus step, you can look into the transition matrix. The transition
matrix ğ‘‡ğ‘‡(ğ‘ ğ‘ , ğ‘ğ‘, ğ‘ ğ‘ â€²) contains probabilities of taking action a from state s then
reaching ğ‘ ğ‘ â€² . Take state 6 as an example:
'''
print(env.env.P[6])